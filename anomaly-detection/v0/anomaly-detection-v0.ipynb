{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9dddb276",
   "metadata": {},
   "source": [
    "# Anomaly Detection\n",
    "\n",
    "```\n",
    "Supervised\n",
    "    dataset 1 - Credit Card Fraud Detection (https://www.kaggle.com/datasets/mlg-ulb/creditcardfraud)\n",
    "    Techniques used and results\n",
    "Unsupervised\n",
    "    dataset 2\n",
    "    Techniques used and results\n",
    "\n",
    "Summary\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "==================================================================================\n",
    "### Types of Anomalies:\n",
    "- Point Anomalies:  \n",
    "    Individual data points that are significantly different from the rest of the data.\n",
    "- Contextual Anomalies:  \n",
    "    Data points that are anomalous only within a specific context (e.g., high network traffic at 3 AM is normal, but at 3 PM might be an anomaly).\n",
    "- Collective Anomalies:  \n",
    "    A group of related data points that collectively represent an anomaly, even if individual points are not anomalous on their own.\n",
    "\n",
    "### Machine Learning Approaches:  \n",
    "Anomaly detection utilizes various machine learning techniques, broadly categorized as:\n",
    "##### Supervised Learning:  \n",
    "Requires labeled datasets with both normal and anomalous data points. Algorithms like: \n",
    "- logistic regression, \n",
    "- decision trees, random forests, and \n",
    "- neural networks can be trained to classify new data as normal or anomalous. This approach is effective when anomalies are well-defined and sufficient labeled data is available. \n",
    "##### Unsupervised Learning:\n",
    "Identifies anomalies without requiring labeled data by learning the underlying patterns of normal data and flagging deviations. This is particularly useful when anomalies are rare or unknown in advance. Common algorithms include:   \n",
    "- Isolation Forest: An ensemble method that isolates anomalies by building a tree structure.  \n",
    "- One-Class SVM (OCSVM): A variant of Support Vector Machines that learns a boundary around normal data, classifying points outside this boundary as anomalies.  \n",
    "- K-Nearest Neighbors (KNN): Anomaly scores are based on the distance to the K-nearest neighbors, with distant points being potential anomalies.   \n",
    "- Autoencoders: Neural networks that learn a compressed representation of data; high reconstruction error can indicate an anomaly.  \n",
    "- Clustering-based methods (e.g., K-Means): Identifies anomalies as data points that do not belong to any cluster or are far from cluster centroids.   \n",
    "##### Semi-supervised Learning:\n",
    "Combines aspects of both supervised and unsupervised learning, often using a small amount of labeled data to guide the learning process.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1170431",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c6d5e38",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "451ff7cd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML-General",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
